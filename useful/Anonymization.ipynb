{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF442: Projet informatique 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDPR in practice: data anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick notebook will show you how to anonymize text data automatically. You'll have to do the same, by testing other approaches and / or embeddings and / or classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some dependencies and some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "from random import sample\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm, trange\n",
    "import sklearn as sk\n",
    "import sklearn.linear_model\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download 3 text files into the \"data\" subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"eng.testa\", \"eng.testb\", \"eng.train\"]\n",
    "url = \"https://raw.githubusercontent.com/glample/tagger/master/dataset\"\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "for file in files:\n",
    "    with open(f\"data/{file}\", 'a') as the_file:\n",
    "        html = urlopen(f\"{url}/{file}\")\n",
    "        for line in html:\n",
    "            the_file.write(line.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words: list, labels: list = []):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "                specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "\n",
    "def read_examples_from_file(data_dir, mode=\"eng.testa\"):\n",
    "    \"\"\"Creating InputExamples out of a file\"\"\"\n",
    "    file_path = os.path.join(data_dir, \"{}\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=1,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\", example.guid)\n",
    "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A direct, pre-trained NER approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code will (almost) do directly what you'll have to accomplish in this *Projet*. Many things are hidden (that you'll have to uncover).\n",
    "\n",
    "*Note* that this will take long the first time because model(s) are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99980503,\n",
       "  'index': 1,\n",
       "  'word': 'Ad',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99964607,\n",
       "  'index': 2,\n",
       "  'word': '##rien',\n",
       "  'start': 2,\n",
       "  'end': 6},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9998079,\n",
       "  'index': 3,\n",
       "  'word': 'E',\n",
       "  'start': 7,\n",
       "  'end': 8},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99225086,\n",
       "  'index': 4,\n",
       "  'word': '##hr',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9994708,\n",
       "  'index': 5,\n",
       "  'word': '##hardt',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.8828938,\n",
       "  'index': 22,\n",
       "  'word': 'IN',\n",
       "  'start': 64,\n",
       "  'end': 66},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.5023682,\n",
       "  'index': 23,\n",
       "  'word': '##F',\n",
       "  'start': 66,\n",
       "  'end': 67}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Adrien Ehrhardt donne un projet d'informatique aux étudiants de INF442.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to detect \"Adrien Ehrhardt\" and anonymize this part. Here it works quite well since input examples 'Ad', '##rien', 'E', '##hr', '##hardt' are being classified as PER. (Even INF is detected as an Organization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the representation of the dataset from Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested above, the whole process of choosing a dictionary, tokenizing the text, learning / obtaining a clever representation and learning / obtaining a classification model on top of that representation is hidden.\n",
    "\n",
    "We'll have a quick look under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a tokenizer, and a model (which needs tokenized inputs). Again, it may take time as probably slightly different tokenizer + model are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer has a vocabulary of size ~ 29k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[unused1]', 1),\n",
       "             ('[unused2]', 2),\n",
       "             ('[unused3]', 3),\n",
       "             ('[unused4]', 4),\n",
       "             ('[unused5]', 5),\n",
       "             ('[unused6]', 6),\n",
       "             ('[unused7]', 7),\n",
       "             ('[unused8]', 8),\n",
       "             ('[unused9]', 9),\n",
       "             ('[unused10]', 10),\n",
       "             ('[unused11]', 11),\n",
       "             ('[unused12]', 12),\n",
       "             ('[unused13]', 13),\n",
       "             ('[unused14]', 14),\n",
       "             ('[unused15]', 15),\n",
       "             ('[unused16]', 16),\n",
       "             ('[unused17]', 17),\n",
       "             ('[unused18]', 18),\n",
       "             ('[unused19]', 19),\n",
       "             ('[unused20]', 20),\n",
       "             ('[unused21]', 21),\n",
       "             ('[unused22]', 22),\n",
       "             ('[unused23]', 23),\n",
       "             ('[unused24]', 24),\n",
       "             ('[unused25]', 25),\n",
       "             ('[unused26]', 26),\n",
       "             ('[unused27]', 27),\n",
       "             ('[unused28]', 28),\n",
       "             ('[unused29]', 29),\n",
       "             ('[unused30]', 30),\n",
       "             ('[unused31]', 31),\n",
       "             ('[unused32]', 32),\n",
       "             ('[unused33]', 33),\n",
       "             ('[unused34]', 34),\n",
       "             ('[unused35]', 35),\n",
       "             ('[unused36]', 36),\n",
       "             ('[unused37]', 37),\n",
       "             ('[unused38]', 38),\n",
       "             ('[unused39]', 39),\n",
       "             ('[unused40]', 40),\n",
       "             ('[unused41]', 41),\n",
       "             ('[unused42]', 42),\n",
       "             ('[unused43]', 43),\n",
       "             ('[unused44]', 44),\n",
       "             ('[unused45]', 45),\n",
       "             ('[unused46]', 46),\n",
       "             ('[unused47]', 47),\n",
       "             ('[unused48]', 48),\n",
       "             ('[unused49]', 49),\n",
       "             ('[unused50]', 50),\n",
       "             ('[unused51]', 51),\n",
       "             ('[unused52]', 52),\n",
       "             ('[unused53]', 53),\n",
       "             ('[unused54]', 54),\n",
       "             ('[unused55]', 55),\n",
       "             ('[unused56]', 56),\n",
       "             ('[unused57]', 57),\n",
       "             ('[unused58]', 58),\n",
       "             ('[unused59]', 59),\n",
       "             ('[unused60]', 60),\n",
       "             ('[unused61]', 61),\n",
       "             ('[unused62]', 62),\n",
       "             ('[unused63]', 63),\n",
       "             ('[unused64]', 64),\n",
       "             ('[unused65]', 65),\n",
       "             ('[unused66]', 66),\n",
       "             ('[unused67]', 67),\n",
       "             ('[unused68]', 68),\n",
       "             ('[unused69]', 69),\n",
       "             ('[unused70]', 70),\n",
       "             ('[unused71]', 71),\n",
       "             ('[unused72]', 72),\n",
       "             ('[unused73]', 73),\n",
       "             ('[unused74]', 74),\n",
       "             ('[unused75]', 75),\n",
       "             ('[unused76]', 76),\n",
       "             ('[unused77]', 77),\n",
       "             ('[unused78]', 78),\n",
       "             ('[unused79]', 79),\n",
       "             ('[unused80]', 80),\n",
       "             ('[unused81]', 81),\n",
       "             ('[unused82]', 82),\n",
       "             ('[unused83]', 83),\n",
       "             ('[unused84]', 84),\n",
       "             ('[unused85]', 85),\n",
       "             ('[unused86]', 86),\n",
       "             ('[unused87]', 87),\n",
       "             ('[unused88]', 88),\n",
       "             ('[unused89]', 89),\n",
       "             ('[unused90]', 90),\n",
       "             ('[unused91]', 91),\n",
       "             ('[unused92]', 92),\n",
       "             ('[unused93]', 93),\n",
       "             ('[unused94]', 94),\n",
       "             ('[unused95]', 95),\n",
       "             ('[unused96]', 96),\n",
       "             ('[unused97]', 97),\n",
       "             ('[unused98]', 98),\n",
       "             ('[unused99]', 99),\n",
       "             ('[UNK]', 100),\n",
       "             ('[CLS]', 101),\n",
       "             ('[SEP]', 102),\n",
       "             ('[MASK]', 103),\n",
       "             ('[unused100]', 104),\n",
       "             ('[unused101]', 105),\n",
       "             ('!', 106),\n",
       "             ('\"', 107),\n",
       "             ('#', 108),\n",
       "             ('$', 109),\n",
       "             ('%', 110),\n",
       "             ('&', 111),\n",
       "             (\"'\", 112),\n",
       "             ('(', 113),\n",
       "             (')', 114),\n",
       "             ('*', 115),\n",
       "             ('+', 116),\n",
       "             (',', 117),\n",
       "             ('-', 118),\n",
       "             ('.', 119),\n",
       "             ('/', 120),\n",
       "             ('0', 121),\n",
       "             ('1', 122),\n",
       "             ('2', 123),\n",
       "             ('3', 124),\n",
       "             ('4', 125),\n",
       "             ('5', 126),\n",
       "             ('6', 127),\n",
       "             ('7', 128),\n",
       "             ('8', 129),\n",
       "             ('9', 130),\n",
       "             (':', 131),\n",
       "             (';', 132),\n",
       "             ('<', 133),\n",
       "             ('=', 134),\n",
       "             ('>', 135),\n",
       "             ('?', 136),\n",
       "             ('@', 137),\n",
       "             ('A', 138),\n",
       "             ('B', 139),\n",
       "             ('C', 140),\n",
       "             ('D', 141),\n",
       "             ('E', 142),\n",
       "             ('F', 143),\n",
       "             ('G', 144),\n",
       "             ('H', 145),\n",
       "             ('I', 146),\n",
       "             ('J', 147),\n",
       "             ('K', 148),\n",
       "             ('L', 149),\n",
       "             ('M', 150),\n",
       "             ('N', 151),\n",
       "             ('O', 152),\n",
       "             ('P', 153),\n",
       "             ('Q', 154),\n",
       "             ('R', 155),\n",
       "             ('S', 156),\n",
       "             ('T', 157),\n",
       "             ('U', 158),\n",
       "             ('V', 159),\n",
       "             ('W', 160),\n",
       "             ('X', 161),\n",
       "             ('Y', 162),\n",
       "             ('Z', 163),\n",
       "             ('[', 164),\n",
       "             ('\\\\', 165),\n",
       "             (']', 166),\n",
       "             ('^', 167),\n",
       "             ('_', 168),\n",
       "             ('`', 169),\n",
       "             ('a', 170),\n",
       "             ('b', 171),\n",
       "             ('c', 172),\n",
       "             ('d', 173),\n",
       "             ('e', 174),\n",
       "             ('f', 175),\n",
       "             ('g', 176),\n",
       "             ('h', 177),\n",
       "             ('i', 178),\n",
       "             ('j', 179),\n",
       "             ('k', 180),\n",
       "             ('l', 181),\n",
       "             ('m', 182),\n",
       "             ('n', 183),\n",
       "             ('o', 184),\n",
       "             ('p', 185),\n",
       "             ('q', 186),\n",
       "             ('r', 187),\n",
       "             ('s', 188),\n",
       "             ('t', 189),\n",
       "             ('u', 190),\n",
       "             ('v', 191),\n",
       "             ('w', 192),\n",
       "             ('x', 193),\n",
       "             ('y', 194),\n",
       "             ('z', 195),\n",
       "             ('{', 196),\n",
       "             ('|', 197),\n",
       "             ('}', 198),\n",
       "             ('~', 199),\n",
       "             ('¡', 200),\n",
       "             ('¢', 201),\n",
       "             ('£', 202),\n",
       "             ('¥', 203),\n",
       "             ('§', 204),\n",
       "             ('¨', 205),\n",
       "             ('©', 206),\n",
       "             ('ª', 207),\n",
       "             ('«', 208),\n",
       "             ('¬', 209),\n",
       "             ('®', 210),\n",
       "             ('°', 211),\n",
       "             ('±', 212),\n",
       "             ('²', 213),\n",
       "             ('³', 214),\n",
       "             ('´', 215),\n",
       "             ('µ', 216),\n",
       "             ('¶', 217),\n",
       "             ('·', 218),\n",
       "             ('¹', 219),\n",
       "             ('º', 220),\n",
       "             ('»', 221),\n",
       "             ('¼', 222),\n",
       "             ('½', 223),\n",
       "             ('¾', 224),\n",
       "             ('¿', 225),\n",
       "             ('À', 226),\n",
       "             ('Á', 227),\n",
       "             ('Â', 228),\n",
       "             ('Ä', 229),\n",
       "             ('Å', 230),\n",
       "             ('Æ', 231),\n",
       "             ('Ç', 232),\n",
       "             ('È', 233),\n",
       "             ('É', 234),\n",
       "             ('Í', 235),\n",
       "             ('Î', 236),\n",
       "             ('Ñ', 237),\n",
       "             ('Ó', 238),\n",
       "             ('Ö', 239),\n",
       "             ('×', 240),\n",
       "             ('Ø', 241),\n",
       "             ('Ú', 242),\n",
       "             ('Ü', 243),\n",
       "             ('Þ', 244),\n",
       "             ('ß', 245),\n",
       "             ('à', 246),\n",
       "             ('á', 247),\n",
       "             ('â', 248),\n",
       "             ('ã', 249),\n",
       "             ('ä', 250),\n",
       "             ('å', 251),\n",
       "             ('æ', 252),\n",
       "             ('ç', 253),\n",
       "             ('è', 254),\n",
       "             ('é', 255),\n",
       "             ('ê', 256),\n",
       "             ('ë', 257),\n",
       "             ('ì', 258),\n",
       "             ('í', 259),\n",
       "             ('î', 260),\n",
       "             ('ï', 261),\n",
       "             ('ð', 262),\n",
       "             ('ñ', 263),\n",
       "             ('ò', 264),\n",
       "             ('ó', 265),\n",
       "             ('ô', 266),\n",
       "             ('õ', 267),\n",
       "             ('ö', 268),\n",
       "             ('÷', 269),\n",
       "             ('ø', 270),\n",
       "             ('ù', 271),\n",
       "             ('ú', 272),\n",
       "             ('û', 273),\n",
       "             ('ü', 274),\n",
       "             ('ý', 275),\n",
       "             ('þ', 276),\n",
       "             ('ÿ', 277),\n",
       "             ('Ā', 278),\n",
       "             ('ā', 279),\n",
       "             ('ă', 280),\n",
       "             ('ą', 281),\n",
       "             ('Ć', 282),\n",
       "             ('ć', 283),\n",
       "             ('Č', 284),\n",
       "             ('č', 285),\n",
       "             ('ď', 286),\n",
       "             ('Đ', 287),\n",
       "             ('đ', 288),\n",
       "             ('ē', 289),\n",
       "             ('ė', 290),\n",
       "             ('ę', 291),\n",
       "             ('ě', 292),\n",
       "             ('ğ', 293),\n",
       "             ('ġ', 294),\n",
       "             ('Ħ', 295),\n",
       "             ('ħ', 296),\n",
       "             ('ĩ', 297),\n",
       "             ('Ī', 298),\n",
       "             ('ī', 299),\n",
       "             ('İ', 300),\n",
       "             ('ı', 301),\n",
       "             ('ļ', 302),\n",
       "             ('Ľ', 303),\n",
       "             ('ľ', 304),\n",
       "             ('Ł', 305),\n",
       "             ('ł', 306),\n",
       "             ('ń', 307),\n",
       "             ('ņ', 308),\n",
       "             ('ň', 309),\n",
       "             ('ŋ', 310),\n",
       "             ('Ō', 311),\n",
       "             ('ō', 312),\n",
       "             ('ŏ', 313),\n",
       "             ('ő', 314),\n",
       "             ('Œ', 315),\n",
       "             ('œ', 316),\n",
       "             ('ř', 317),\n",
       "             ('Ś', 318),\n",
       "             ('ś', 319),\n",
       "             ('Ş', 320),\n",
       "             ('ş', 321),\n",
       "             ('Š', 322),\n",
       "             ('š', 323),\n",
       "             ('Ţ', 324),\n",
       "             ('ţ', 325),\n",
       "             ('ť', 326),\n",
       "             ('ũ', 327),\n",
       "             ('ū', 328),\n",
       "             ('ŭ', 329),\n",
       "             ('ů', 330),\n",
       "             ('ű', 331),\n",
       "             ('ų', 332),\n",
       "             ('ŵ', 333),\n",
       "             ('ŷ', 334),\n",
       "             ('ź', 335),\n",
       "             ('Ż', 336),\n",
       "             ('ż', 337),\n",
       "             ('Ž', 338),\n",
       "             ('ž', 339),\n",
       "             ('Ə', 340),\n",
       "             ('ƒ', 341),\n",
       "             ('ơ', 342),\n",
       "             ('ư', 343),\n",
       "             ('ǎ', 344),\n",
       "             ('ǐ', 345),\n",
       "             ('ǒ', 346),\n",
       "             ('ǔ', 347),\n",
       "             ('ǫ', 348),\n",
       "             ('Ș', 349),\n",
       "             ('ș', 350),\n",
       "             ('Ț', 351),\n",
       "             ('ț', 352),\n",
       "             ('ɐ', 353),\n",
       "             ('ɑ', 354),\n",
       "             ('ɔ', 355),\n",
       "             ('ɕ', 356),\n",
       "             ('ə', 357),\n",
       "             ('ɛ', 358),\n",
       "             ('ɡ', 359),\n",
       "             ('ɣ', 360),\n",
       "             ('ɨ', 361),\n",
       "             ('ɪ', 362),\n",
       "             ('ɲ', 363),\n",
       "             ('ɾ', 364),\n",
       "             ('ʀ', 365),\n",
       "             ('ʁ', 366),\n",
       "             ('ʂ', 367),\n",
       "             ('ʃ', 368),\n",
       "             ('ʊ', 369),\n",
       "             ('ʋ', 370),\n",
       "             ('ʌ', 371),\n",
       "             ('ʐ', 372),\n",
       "             ('ʑ', 373),\n",
       "             ('ʒ', 374),\n",
       "             ('ʔ', 375),\n",
       "             ('ʰ', 376),\n",
       "             ('ʲ', 377),\n",
       "             ('ʳ', 378),\n",
       "             ('ʷ', 379),\n",
       "             ('ʻ', 380),\n",
       "             ('ʼ', 381),\n",
       "             ('ʾ', 382),\n",
       "             ('ʿ', 383),\n",
       "             ('ˈ', 384),\n",
       "             ('ː', 385),\n",
       "             ('ˡ', 386),\n",
       "             ('ˢ', 387),\n",
       "             ('ˣ', 388),\n",
       "             ('́', 389),\n",
       "             ('̃', 390),\n",
       "             ('̍', 391),\n",
       "             ('̯', 392),\n",
       "             ('͡', 393),\n",
       "             ('Α', 394),\n",
       "             ('Β', 395),\n",
       "             ('Γ', 396),\n",
       "             ('Δ', 397),\n",
       "             ('Ε', 398),\n",
       "             ('Η', 399),\n",
       "             ('Θ', 400),\n",
       "             ('Ι', 401),\n",
       "             ('Κ', 402),\n",
       "             ('Λ', 403),\n",
       "             ('Μ', 404),\n",
       "             ('Ν', 405),\n",
       "             ('Ο', 406),\n",
       "             ('Π', 407),\n",
       "             ('Σ', 408),\n",
       "             ('Τ', 409),\n",
       "             ('Φ', 410),\n",
       "             ('Χ', 411),\n",
       "             ('Ψ', 412),\n",
       "             ('Ω', 413),\n",
       "             ('ά', 414),\n",
       "             ('έ', 415),\n",
       "             ('ή', 416),\n",
       "             ('ί', 417),\n",
       "             ('α', 418),\n",
       "             ('β', 419),\n",
       "             ('γ', 420),\n",
       "             ('δ', 421),\n",
       "             ('ε', 422),\n",
       "             ('ζ', 423),\n",
       "             ('η', 424),\n",
       "             ('θ', 425),\n",
       "             ('ι', 426),\n",
       "             ('κ', 427),\n",
       "             ('λ', 428),\n",
       "             ('μ', 429),\n",
       "             ('ν', 430),\n",
       "             ('ξ', 431),\n",
       "             ('ο', 432),\n",
       "             ('π', 433),\n",
       "             ('ρ', 434),\n",
       "             ('ς', 435),\n",
       "             ('σ', 436),\n",
       "             ('τ', 437),\n",
       "             ('υ', 438),\n",
       "             ('φ', 439),\n",
       "             ('χ', 440),\n",
       "             ('ψ', 441),\n",
       "             ('ω', 442),\n",
       "             ('ό', 443),\n",
       "             ('ύ', 444),\n",
       "             ('ώ', 445),\n",
       "             ('І', 446),\n",
       "             ('Ј', 447),\n",
       "             ('А', 448),\n",
       "             ('Б', 449),\n",
       "             ('В', 450),\n",
       "             ('Г', 451),\n",
       "             ('Д', 452),\n",
       "             ('Е', 453),\n",
       "             ('Ж', 454),\n",
       "             ('З', 455),\n",
       "             ('И', 456),\n",
       "             ('К', 457),\n",
       "             ('Л', 458),\n",
       "             ('М', 459),\n",
       "             ('Н', 460),\n",
       "             ('О', 461),\n",
       "             ('П', 462),\n",
       "             ('Р', 463),\n",
       "             ('С', 464),\n",
       "             ('Т', 465),\n",
       "             ('У', 466),\n",
       "             ('Ф', 467),\n",
       "             ('Х', 468),\n",
       "             ('Ц', 469),\n",
       "             ('Ч', 470),\n",
       "             ('Ш', 471),\n",
       "             ('Э', 472),\n",
       "             ('Ю', 473),\n",
       "             ('Я', 474),\n",
       "             ('а', 475),\n",
       "             ('б', 476),\n",
       "             ('в', 477),\n",
       "             ('г', 478),\n",
       "             ('д', 479),\n",
       "             ('е', 480),\n",
       "             ('ж', 481),\n",
       "             ('з', 482),\n",
       "             ('и', 483),\n",
       "             ('й', 484),\n",
       "             ('к', 485),\n",
       "             ('л', 486),\n",
       "             ('м', 487),\n",
       "             ('н', 488),\n",
       "             ('о', 489),\n",
       "             ('п', 490),\n",
       "             ('р', 491),\n",
       "             ('с', 492),\n",
       "             ('т', 493),\n",
       "             ('у', 494),\n",
       "             ('ф', 495),\n",
       "             ('х', 496),\n",
       "             ('ц', 497),\n",
       "             ('ч', 498),\n",
       "             ('ш', 499),\n",
       "             ('щ', 500),\n",
       "             ('ъ', 501),\n",
       "             ('ы', 502),\n",
       "             ('ь', 503),\n",
       "             ('э', 504),\n",
       "             ('ю', 505),\n",
       "             ('я', 506),\n",
       "             ('ё', 507),\n",
       "             ('і', 508),\n",
       "             ('ї', 509),\n",
       "             ('ј', 510),\n",
       "             ('њ', 511),\n",
       "             ('ћ', 512),\n",
       "             ('Ա', 513),\n",
       "             ('Հ', 514),\n",
       "             ('ա', 515),\n",
       "             ('ե', 516),\n",
       "             ('ի', 517),\n",
       "             ('կ', 518),\n",
       "             ('մ', 519),\n",
       "             ('յ', 520),\n",
       "             ('ն', 521),\n",
       "             ('ո', 522),\n",
       "             ('ս', 523),\n",
       "             ('տ', 524),\n",
       "             ('ր', 525),\n",
       "             ('ւ', 526),\n",
       "             ('ְ', 527),\n",
       "             ('ִ', 528),\n",
       "             ('ֵ', 529),\n",
       "             ('ֶ', 530),\n",
       "             ('ַ', 531),\n",
       "             ('ָ', 532),\n",
       "             ('ֹ', 533),\n",
       "             ('ּ', 534),\n",
       "             ('א', 535),\n",
       "             ('ב', 536),\n",
       "             ('ג', 537),\n",
       "             ('ד', 538),\n",
       "             ('ה', 539),\n",
       "             ('ו', 540),\n",
       "             ('ז', 541),\n",
       "             ('ח', 542),\n",
       "             ('ט', 543),\n",
       "             ('י', 544),\n",
       "             ('כ', 545),\n",
       "             ('ל', 546),\n",
       "             ('ם', 547),\n",
       "             ('מ', 548),\n",
       "             ('ן', 549),\n",
       "             ('נ', 550),\n",
       "             ('ס', 551),\n",
       "             ('ע', 552),\n",
       "             ('פ', 553),\n",
       "             ('צ', 554),\n",
       "             ('ק', 555),\n",
       "             ('ר', 556),\n",
       "             ('ש', 557),\n",
       "             ('ת', 558),\n",
       "             ('،', 559),\n",
       "             ('ء', 560),\n",
       "             ('آ', 561),\n",
       "             ('أ', 562),\n",
       "             ('إ', 563),\n",
       "             ('ئ', 564),\n",
       "             ('ا', 565),\n",
       "             ('ب', 566),\n",
       "             ('ة', 567),\n",
       "             ('ت', 568),\n",
       "             ('ث', 569),\n",
       "             ('ج', 570),\n",
       "             ('ح', 571),\n",
       "             ('خ', 572),\n",
       "             ('د', 573),\n",
       "             ('ذ', 574),\n",
       "             ('ر', 575),\n",
       "             ('ز', 576),\n",
       "             ('س', 577),\n",
       "             ('ش', 578),\n",
       "             ('ص', 579),\n",
       "             ('ض', 580),\n",
       "             ('ط', 581),\n",
       "             ('ظ', 582),\n",
       "             ('ع', 583),\n",
       "             ('غ', 584),\n",
       "             ('ف', 585),\n",
       "             ('ق', 586),\n",
       "             ('ك', 587),\n",
       "             ('ل', 588),\n",
       "             ('م', 589),\n",
       "             ('ن', 590),\n",
       "             ('ه', 591),\n",
       "             ('و', 592),\n",
       "             ('ى', 593),\n",
       "             ('ي', 594),\n",
       "             ('َ', 595),\n",
       "             ('ِ', 596),\n",
       "             ('ٹ', 597),\n",
       "             ('پ', 598),\n",
       "             ('چ', 599),\n",
       "             ('ک', 600),\n",
       "             ('گ', 601),\n",
       "             ('ہ', 602),\n",
       "             ('ی', 603),\n",
       "             ('ے', 604),\n",
       "             ('ं', 605),\n",
       "             ('आ', 606),\n",
       "             ('क', 607),\n",
       "             ('ग', 608),\n",
       "             ('च', 609),\n",
       "             ('ज', 610),\n",
       "             ('ण', 611),\n",
       "             ('त', 612),\n",
       "             ('द', 613),\n",
       "             ('ध', 614),\n",
       "             ('न', 615),\n",
       "             ('प', 616),\n",
       "             ('ब', 617),\n",
       "             ('भ', 618),\n",
       "             ('म', 619),\n",
       "             ('य', 620),\n",
       "             ('र', 621),\n",
       "             ('ल', 622),\n",
       "             ('व', 623),\n",
       "             ('श', 624),\n",
       "             ('ष', 625),\n",
       "             ('स', 626),\n",
       "             ('ह', 627),\n",
       "             ('ा', 628),\n",
       "             ('ि', 629),\n",
       "             ('ी', 630),\n",
       "             ('ु', 631),\n",
       "             ('े', 632),\n",
       "             ('ो', 633),\n",
       "             ('्', 634),\n",
       "             ('।', 635),\n",
       "             ('॥', 636),\n",
       "             ('আ', 637),\n",
       "             ('ই', 638),\n",
       "             ('এ', 639),\n",
       "             ('ও', 640),\n",
       "             ('ক', 641),\n",
       "             ('খ', 642),\n",
       "             ('গ', 643),\n",
       "             ('চ', 644),\n",
       "             ('ছ', 645),\n",
       "             ('জ', 646),\n",
       "             ('ট', 647),\n",
       "             ('ত', 648),\n",
       "             ('থ', 649),\n",
       "             ('দ', 650),\n",
       "             ('ধ', 651),\n",
       "             ('ন', 652),\n",
       "             ('প', 653),\n",
       "             ('ব', 654),\n",
       "             ('ম', 655),\n",
       "             ('য', 656),\n",
       "             ('র', 657),\n",
       "             ('ল', 658),\n",
       "             ('শ', 659),\n",
       "             ('স', 660),\n",
       "             ('হ', 661),\n",
       "             ('়', 662),\n",
       "             ('া', 663),\n",
       "             ('ি', 664),\n",
       "             ('ী', 665),\n",
       "             ('ু', 666),\n",
       "             ('ে', 667),\n",
       "             ('ো', 668),\n",
       "             ('্', 669),\n",
       "             ('য়', 670),\n",
       "             ('க', 671),\n",
       "             ('த', 672),\n",
       "             ('ப', 673),\n",
       "             ('ம', 674),\n",
       "             ('ய', 675),\n",
       "             ('ர', 676),\n",
       "             ('ல', 677),\n",
       "             ('வ', 678),\n",
       "             ('ா', 679),\n",
       "             ('ி', 680),\n",
       "             ('ு', 681),\n",
       "             ('்', 682),\n",
       "             ('ร', 683),\n",
       "             ('་', 684),\n",
       "             ('ག', 685),\n",
       "             ('ང', 686),\n",
       "             ('ད', 687),\n",
       "             ('ན', 688),\n",
       "             ('བ', 689),\n",
       "             ('མ', 690),\n",
       "             ('ར', 691),\n",
       "             ('ལ', 692),\n",
       "             ('ས', 693),\n",
       "             ('ི', 694),\n",
       "             ('ུ', 695),\n",
       "             ('ེ', 696),\n",
       "             ('ོ', 697),\n",
       "             ('ა', 698),\n",
       "             ('ე', 699),\n",
       "             ('ი', 700),\n",
       "             ('ლ', 701),\n",
       "             ('ნ', 702),\n",
       "             ('ო', 703),\n",
       "             ('რ', 704),\n",
       "             ('ს', 705),\n",
       "             ('ᴬ', 706),\n",
       "             ('ᴵ', 707),\n",
       "             ('ᵀ', 708),\n",
       "             ('ᵃ', 709),\n",
       "             ('ᵇ', 710),\n",
       "             ('ᵈ', 711),\n",
       "             ('ᵉ', 712),\n",
       "             ('ᵍ', 713),\n",
       "             ('ᵏ', 714),\n",
       "             ('ᵐ', 715),\n",
       "             ('ᵒ', 716),\n",
       "             ('ᵖ', 717),\n",
       "             ('ᵗ', 718),\n",
       "             ('ᵘ', 719),\n",
       "             ('ᵢ', 720),\n",
       "             ('ᵣ', 721),\n",
       "             ('ᵤ', 722),\n",
       "             ('ᵥ', 723),\n",
       "             ('ᶜ', 724),\n",
       "             ('ᶠ', 725),\n",
       "             ('ḍ', 726),\n",
       "             ('Ḥ', 727),\n",
       "             ('ḥ', 728),\n",
       "             ('Ḩ', 729),\n",
       "             ('ḩ', 730),\n",
       "             ('ḳ', 731),\n",
       "             ('ṃ', 732),\n",
       "             ('ṅ', 733),\n",
       "             ('ṇ', 734),\n",
       "             ('ṛ', 735),\n",
       "             ('ṣ', 736),\n",
       "             ('ṭ', 737),\n",
       "             ('ạ', 738),\n",
       "             ('ả', 739),\n",
       "             ('ấ', 740),\n",
       "             ('ầ', 741),\n",
       "             ('ẩ', 742),\n",
       "             ('ậ', 743),\n",
       "             ('ắ', 744),\n",
       "             ('ế', 745),\n",
       "             ('ề', 746),\n",
       "             ('ể', 747),\n",
       "             ('ễ', 748),\n",
       "             ('ệ', 749),\n",
       "             ('ị', 750),\n",
       "             ('ọ', 751),\n",
       "             ('ố', 752),\n",
       "             ('ồ', 753),\n",
       "             ('ổ', 754),\n",
       "             ('ộ', 755),\n",
       "             ('ớ', 756),\n",
       "             ('ờ', 757),\n",
       "             ('ợ', 758),\n",
       "             ('ụ', 759),\n",
       "             ('ủ', 760),\n",
       "             ('ứ', 761),\n",
       "             ('ừ', 762),\n",
       "             ('ử', 763),\n",
       "             ('ữ', 764),\n",
       "             ('ự', 765),\n",
       "             ('ỳ', 766),\n",
       "             ('ỹ', 767),\n",
       "             ('ἀ', 768),\n",
       "             ('ἐ', 769),\n",
       "             ('ὁ', 770),\n",
       "             ('ὐ', 771),\n",
       "             ('ὰ', 772),\n",
       "             ('ὶ', 773),\n",
       "             ('ὸ', 774),\n",
       "             ('ῆ', 775),\n",
       "             ('ῖ', 776),\n",
       "             ('ῦ', 777),\n",
       "             ('ῶ', 778),\n",
       "             ('‐', 779),\n",
       "             ('‑', 780),\n",
       "             ('‒', 781),\n",
       "             ('–', 782),\n",
       "             ('—', 783),\n",
       "             ('―', 784),\n",
       "             ('‖', 785),\n",
       "             ('‘', 786),\n",
       "             ('’', 787),\n",
       "             ('‚', 788),\n",
       "             ('“', 789),\n",
       "             ('”', 790),\n",
       "             ('„', 791),\n",
       "             ('†', 792),\n",
       "             ('‡', 793),\n",
       "             ('•', 794),\n",
       "             ('…', 795),\n",
       "             ('‰', 796),\n",
       "             ('′', 797),\n",
       "             ('″', 798),\n",
       "             ('⁄', 799),\n",
       "             ('⁰', 800),\n",
       "             ('ⁱ', 801),\n",
       "             ('⁴', 802),\n",
       "             ('⁵', 803),\n",
       "             ('⁶', 804),\n",
       "             ('⁷', 805),\n",
       "             ('⁸', 806),\n",
       "             ('⁹', 807),\n",
       "             ('⁺', 808),\n",
       "             ('⁻', 809),\n",
       "             ('ⁿ', 810),\n",
       "             ('₀', 811),\n",
       "             ('₁', 812),\n",
       "             ('₂', 813),\n",
       "             ('₃', 814),\n",
       "             ('₄', 815),\n",
       "             ('₅', 816),\n",
       "             ('₆', 817),\n",
       "             ('₇', 818),\n",
       "             ('₈', 819),\n",
       "             ('₉', 820),\n",
       "             ('₊', 821),\n",
       "             ('₍', 822),\n",
       "             ('₎', 823),\n",
       "             ('ₐ', 824),\n",
       "             ('ₑ', 825),\n",
       "             ('ₒ', 826),\n",
       "             ('ₓ', 827),\n",
       "             ('ₕ', 828),\n",
       "             ('ₖ', 829),\n",
       "             ('ₘ', 830),\n",
       "             ('ₙ', 831),\n",
       "             ('ₚ', 832),\n",
       "             ('ₛ', 833),\n",
       "             ('ₜ', 834),\n",
       "             ('₤', 835),\n",
       "             ('€', 836),\n",
       "             ('₱', 837),\n",
       "             ('₹', 838),\n",
       "             ('ℓ', 839),\n",
       "             ('№', 840),\n",
       "             ('ℝ', 841),\n",
       "             ('⅓', 842),\n",
       "             ('←', 843),\n",
       "             ('↑', 844),\n",
       "             ('→', 845),\n",
       "             ('↔', 846),\n",
       "             ('⇌', 847),\n",
       "             ('⇒', 848),\n",
       "             ('∂', 849),\n",
       "             ('∈', 850),\n",
       "             ('−', 851),\n",
       "             ('∗', 852),\n",
       "             ('∘', 853),\n",
       "             ('√', 854),\n",
       "             ('∞', 855),\n",
       "             ('∧', 856),\n",
       "             ('∨', 857),\n",
       "             ('∩', 858),\n",
       "             ('∪', 859),\n",
       "             ('≈', 860),\n",
       "             ('≠', 861),\n",
       "             ('≡', 862),\n",
       "             ('≤', 863),\n",
       "             ('≥', 864),\n",
       "             ('⊂', 865),\n",
       "             ('⊆', 866),\n",
       "             ('⊕', 867),\n",
       "             ('⋅', 868),\n",
       "             ('─', 869),\n",
       "             ('│', 870),\n",
       "             ('■', 871),\n",
       "             ('●', 872),\n",
       "             ('★', 873),\n",
       "             ('☆', 874),\n",
       "             ('☉', 875),\n",
       "             ('♠', 876),\n",
       "             ('♣', 877),\n",
       "             ('♥', 878),\n",
       "             ('♦', 879),\n",
       "             ('♭', 880),\n",
       "             ('♯', 881),\n",
       "             ('⟨', 882),\n",
       "             ('⟩', 883),\n",
       "             ('ⱼ', 884),\n",
       "             ('、', 885),\n",
       "             ('。', 886),\n",
       "             ('《', 887),\n",
       "             ('》', 888),\n",
       "             ('「', 889),\n",
       "             ('」', 890),\n",
       "             ('『', 891),\n",
       "             ('』', 892),\n",
       "             ('〜', 893),\n",
       "             ('い', 894),\n",
       "             ('う', 895),\n",
       "             ('え', 896),\n",
       "             ('お', 897),\n",
       "             ('か', 898),\n",
       "             ('き', 899),\n",
       "             ('く', 900),\n",
       "             ('け', 901),\n",
       "             ('こ', 902),\n",
       "             ('さ', 903),\n",
       "             ('し', 904),\n",
       "             ('す', 905),\n",
       "             ('せ', 906),\n",
       "             ('そ', 907),\n",
       "             ('た', 908),\n",
       "             ('ち', 909),\n",
       "             ('つ', 910),\n",
       "             ('て', 911),\n",
       "             ('と', 912),\n",
       "             ('な', 913),\n",
       "             ('に', 914),\n",
       "             ('の', 915),\n",
       "             ('は', 916),\n",
       "             ('ひ', 917),\n",
       "             ('ま', 918),\n",
       "             ('み', 919),\n",
       "             ('む', 920),\n",
       "             ('め', 921),\n",
       "             ('も', 922),\n",
       "             ('や', 923),\n",
       "             ('ゆ', 924),\n",
       "             ('よ', 925),\n",
       "             ('ら', 926),\n",
       "             ('り', 927),\n",
       "             ('る', 928),\n",
       "             ('れ', 929),\n",
       "             ('ん', 930),\n",
       "             ('ア', 931),\n",
       "             ('ィ', 932),\n",
       "             ('イ', 933),\n",
       "             ('ウ', 934),\n",
       "             ('エ', 935),\n",
       "             ('オ', 936),\n",
       "             ('カ', 937),\n",
       "             ('ガ', 938),\n",
       "             ('キ', 939),\n",
       "             ('ク', 940),\n",
       "             ('グ', 941),\n",
       "             ('コ', 942),\n",
       "             ('サ', 943),\n",
       "             ('シ', 944),\n",
       "             ('ジ', 945),\n",
       "             ('ス', 946),\n",
       "             ('ズ', 947),\n",
       "             ('タ', 948),\n",
       "             ('ダ', 949),\n",
       "             ('ッ', 950),\n",
       "             ('テ', 951),\n",
       "             ('デ', 952),\n",
       "             ('ト', 953),\n",
       "             ('ド', 954),\n",
       "             ('ナ', 955),\n",
       "             ('ニ', 956),\n",
       "             ('ハ', 957),\n",
       "             ('バ', 958),\n",
       "             ('パ', 959),\n",
       "             ('フ', 960),\n",
       "             ('ブ', 961),\n",
       "             ('プ', 962),\n",
       "             ('マ', 963),\n",
       "             ('ミ', 964),\n",
       "             ('ム', 965),\n",
       "             ('ャ', 966),\n",
       "             ('ュ', 967),\n",
       "             ('ラ', 968),\n",
       "             ('リ', 969),\n",
       "             ('ル', 970),\n",
       "             ('レ', 971),\n",
       "             ('ロ', 972),\n",
       "             ('ン', 973),\n",
       "             ('・', 974),\n",
       "             ('ー', 975),\n",
       "             ('一', 976),\n",
       "             ('三', 977),\n",
       "             ('上', 978),\n",
       "             ('下', 979),\n",
       "             ('中', 980),\n",
       "             ('事', 981),\n",
       "             ('二', 982),\n",
       "             ('井', 983),\n",
       "             ('京', 984),\n",
       "             ('人', 985),\n",
       "             ('亻', 986),\n",
       "             ('仁', 987),\n",
       "             ('佐', 988),\n",
       "             ('侍', 989),\n",
       "             ('光', 990),\n",
       "             ('公', 991),\n",
       "             ('力', 992),\n",
       "             ('北', 993),\n",
       "             ('十', 994),\n",
       "             ('南', 995),\n",
       "             ('原', 996),\n",
       "             ('口', 997),\n",
       "             ('史', 998),\n",
       "             ('司', 999),\n",
       "             ...])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Adrien Ehrhardt donne un projet d'informatique aux étudiants de INF442.\"\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inputs` is the index of the tokens (as if you'd keep track of the page number in a Larousse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 24930, 22500,   142,  8167, 16464,  1274,  1673,  8362,  5250,\n",
       "         18836,   173,   112, 12862, 11745,  3530, 24544,   255,  7926, 10359,\n",
       "          2145,  1260, 15969,  2271, 25041,  1477,   119,   102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(tokenizer.vocab.items())\n",
    "inputs_np = inputs.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('Ad', 24930)\n",
      "('##rien', 22500)\n",
      "('E', 142)\n",
      "('##hr', 8167)\n",
      "('##hardt', 16464)\n",
      "('don', 1274)\n",
      "('##ne', 1673)\n",
      "('un', 8362)\n",
      "('pro', 5250)\n",
      "('##jet', 18836)\n",
      "('d', 173)\n",
      "(\"'\", 112)\n",
      "('inform', 12862)\n",
      "('##ati', 11745)\n",
      "('##que', 3530)\n",
      "('aux', 24544)\n",
      "('é', 255)\n",
      "('##tu', 7926)\n",
      "('##dian', 10359)\n",
      "('##ts', 2145)\n",
      "('de', 1260)\n",
      "('IN', 15969)\n",
      "('##F', 2271)\n",
      "('##44', 25041)\n",
      "('##2', 1477)\n",
      "('.', 119)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "for index in inputs_np:\n",
    "    print(dictionary[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the resulting input into the model and retrieve the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.detach().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4247,  0.1692,  0.0766,  ...,  0.1469,  0.6186, -0.0278],\n",
       "         [ 0.6228, -0.3287,  0.4009,  ..., -0.1692,  0.2634, -0.0449],\n",
       "         [-0.0646,  0.1704,  0.3089,  ...,  0.3286,  0.4970, -0.1050],\n",
       "         ...,\n",
       "         [ 0.0887,  0.0777,  0.3091,  ...,  0.6490,  0.6358,  0.0915],\n",
       "         [-0.0842,  0.3524,  0.2369,  ...,  0.4639,  0.5785,  0.3579],\n",
       "         [ 0.0724,  0.0754, -0.7835,  ..., -0.1286,  1.3864,  0.1951]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is the 768-dimensional representation of each of the 28 tokens in the sentence (out of a vocabulary size of ~29k)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The real thing: doing the same for the CoNLL03 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the output of the model, *i.e.* BERT's learned representation, of all the datasets we've downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"eng.testb\"  # change this to eng.testa / eng.testb to generate the corresponding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "# The labels in CoNLL03\n",
    "labels = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "# The model we'll use\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")  # Note: you might want to play with the model and associated Tokenizer!\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# Reading the file\n",
    "examples = read_examples_from_file(\".\", mode=f\"data/{dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the words in tokens\n",
    "features = convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list=labels,\n",
    "    max_seq_length=128,\n",
    "    tokenizer=tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=tokenizer.cls_token,\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=tokenizer.sep_token,\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=pad_token_label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensors and build dataset\n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "tensordataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is too computationally intensive, we'll do this by batch of 1 token to easily \"stack\"\n",
    "# the tokens' representation (but it's not computationally efficient - although the difference in \n",
    "# performance will be more significant using a GPU)\n",
    "eval_sampler = SequentialSampler(tensordataset)\n",
    "eval_dataloader = DataLoader(tensordataset, sampler=eval_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████| 24171/24171 [1:12:22<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "out_label_ids = []\n",
    "# This tells the model to only \"evaluate\" (forward-pass)\n",
    "model.eval()\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    # batch = tuple(t.to(\"gpu\") for t in batch)  # if you have a compatible GPU, uncomment this\n",
    "    with torch.no_grad():  # do not calculate gradients, as we won't be doing backward propagation\n",
    "        inputs = {\"input_ids\": batch[0], \n",
    "                  \"attention_mask\": batch[1],\n",
    "                  \"token_type_ids\": batch[2]}\n",
    "        outputs = model(**inputs)[0].detach().cpu().numpy()  # last hidden layer from tensor to numpy array\n",
    "        preds.append(outputs[0, (batch[3] != pad_token_label_id).cpu().numpy()[0], :])  # ditch padding\n",
    "        out_label_ids.append(batch[3][0][(batch[3] != pad_token_label_id).cpu().numpy()[0]].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of 1D arrays to 2D arrays\n",
    "# Each row represents a token\n",
    "# We have lost the notion of sentence (but we don't care here)\n",
    "# For other downstream tasks, such as Passage Retrieval, Question Answering,\n",
    "# ... knowing the surrounding sentences is an important information.\n",
    "representation = np.concatenate(preds, axis=0)\n",
    "out_label_ids = np.concatenate(out_label_ids)\n",
    "label_map = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label index (0, ..., 6) -> label ('O', 'PERS', ...)\n",
    "def remap(x):\n",
    "    return label_map[x]\n",
    "\n",
    "vf = np.vectorize(remap)\n",
    "true_labels = vf(out_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'I-LOC', ..., 'O', 'I-PER', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324919,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26978433, -0.39845598,  0.40575168, ...,  0.01292597,\n",
       "         0.24568917,  0.04700899],\n",
       "       [-0.22996216, -0.19745646,  0.56731385, ...,  0.42715392,\n",
       "         0.40921098, -0.06547163],\n",
       "       [ 0.1031662 , -0.22714055,  0.72148806, ...,  0.29335234,\n",
       "         0.70935506, -0.35829127],\n",
       "       ...,\n",
       "       [ 0.56735927,  0.4612093 ,  0.23810202, ...,  0.33974737,\n",
       "        -0.7844839 ,  0.37409455],\n",
       "       [-0.34889153, -0.24226657,  0.29281753, ...,  0.30067262,\n",
       "         0.12375018,  0.5540694 ],\n",
       "       [ 0.22828603, -0.7856831 , -0.08802494, ...,  0.24558005,\n",
       "        -0.22777386,  0.10435171]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324919, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert true_labels.shape[0] == representation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"data/true_labels.{dataset}.npy\", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"data/representation.{dataset}.npy\", representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Housekeeping\n",
    "del preds, out_label_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a simple classifier on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've covered the hard part (converting words to a useful numerical representation), we can classify them as we would for iris flowers (e.g. with a Logistic Regression - here with cross-validation)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: **yes**, these 3 lines are a satisfactory solution to Subproblem 1 (with performance metrics and comments obviously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top = sk.linear_model.LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/adrien/.pyenv/versions/3.8.12/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.fit(X=representation, y=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999980515241952"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.score(X=representation, y=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use some Python in your project, here's how you can load the numpy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.load(\"data/true_labels.eng.train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I-ORG', 'O', 'I-MISC', ..., 'O', 'I-ORG', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = np.load(\"data/representation.eng.train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26978433, -0.39845598,  0.40575168, ...,  0.01292597,\n",
       "         0.24568917,  0.04700899],\n",
       "       [-0.22996216, -0.19745646,  0.56731385, ...,  0.42715392,\n",
       "         0.40921098, -0.06547163],\n",
       "       [ 0.1031662 , -0.22714055,  0.72148806, ...,  0.29335234,\n",
       "         0.70935506, -0.35829127],\n",
       "       ...,\n",
       "       [ 0.56735927,  0.4612093 ,  0.23810202, ...,  0.33974737,\n",
       "        -0.7844839 ,  0.37409455],\n",
       "       [-0.34889153, -0.24226657,  0.29281753, ...,  0.30067262,\n",
       "         0.12375018,  0.5540694 ],\n",
       "       [ 0.22828603, -0.7856831 , -0.08802494, ...,  0.24558005,\n",
       "        -0.22777386,  0.10435171]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and converting to CSV: files get too big, you'll have to do this yourself and sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    loaded = np.load(f\"data/representation.{file}.npy\")\n",
    "    n_samples = 10000 if file == \"eng.train\" else 2000\n",
    "    samples = sample(list(range(loaded.shape[0])), n_samples)\n",
    "    np.savetxt(f\"data/representation.{file}.csv\", loaded[samples,:], delimiter=\",\")\n",
    "    true_labels = np.load(f\"data/true_labels.{file}.npy\")\n",
    "    np.savetxt(f\"data/true_labels.{file}.csv\", true_labels[samples], delimiter=\",\", fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (INF442)",
   "language": "python",
   "name": "inf442_pi8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
